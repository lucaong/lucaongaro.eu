<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/blog</id>
  <link href="http://blog.url.com/blog"/>
  <link href="http://blog.url.com/blog/feed.xml" rel="self"/>
  <updated>2018-07-31T15:34:00+02:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Delayed jobs with Rails and RabbitMQ</title>
    <link rel="alternate" href="http://blog.url.com/blog/2018/07/31/delayed-jobs-with-rails-and-rabbitmq.html"/>
    <id>http://blog.url.com/blog/2018/07/31/delayed-jobs-with-rails-and-rabbitmq.html</id>
    <published>2018-07-31T15:34:00+02:00</published>
    <updated>2018-08-01T23:54:02+02:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;I recently had the need to schedule background jobs with a specified delay into
the future from a Ruby on Rails application. I had to implement a retry
mechanism with backoff, so I needed to be able to express something like
&amp;ldquo;execute job X, but wait Y seconds before doing so&amp;rdquo;. Clearly, I needed this
mechanism to be non-blocking: if a job is scheduled 5 minutes in the future, the
workers should be free to process other jobs in the meantime.&lt;/p&gt;

&lt;p&gt;Some popular &lt;code&gt;ActiveJob&lt;/code&gt; adapters like Resque or Sidekiq implement this feature,
which is exposed in the &lt;code&gt;ActiveJob&lt;/code&gt; API as the &lt;code&gt;wait: &amp;lt;seconds&amp;gt;&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;table class="rouge-table"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="rouge-gutter gl"&gt;&lt;pre class="lineno"&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class="rouge-code"&gt;&lt;pre&gt;&lt;span class="no"&gt;SomeJob&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;perform_later&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;some_argument&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;wait: &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;minutes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My adapter of choice though is &lt;a href="http://jondot.github.io/sneakers/"&gt;Sneakers&lt;/a&gt;,
which is based on the superb &lt;a href="https://www.rabbitmq.com"&gt;RabbitMQ&lt;/a&gt;.
Unfortunately, as of July 2018, the Sneakers adapter does not implement delayed
jobs out of the box, as the feature table for &lt;code&gt;ActiveJob::QueueAdapters&lt;/code&gt;
dutyfully reports (copied from the &lt;a href="https://api.rubyonrails.org/classes/ActiveJob/QueueAdapters.html"&gt;official
docs&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&lt;table class="rouge-table"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="rouge-gutter gl"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class="rouge-code"&gt;&lt;pre&gt;|                   | Async | Queues | Delayed    | Priorities | Timeout | Retries |
|-------------------|-------|--------|------------|------------|---------|---------|
| Backburner        | Yes   | Yes    | Yes        | Yes        | Job     | Global  |
| Delayed Job       | Yes   | Yes    | Yes        | Job        | Global  | Global  |
| Qu                | Yes   | Yes    | No         | No         | No      | Global  |
| Que               | Yes   | Yes    | Yes        | Job        | No      | Job     |
| queue_classic     | Yes   | Yes    | Yes*       | No         | No      | No      |
| Resque            | Yes   | Yes    | Yes (Gem)  | Queue      | Global  | Yes     |
| Sidekiq           | Yes   | Yes    | Yes        | Queue      | No      | Job     |
| Sneakers          | Yes   | Yes    | No         | Queue      | Queue   | No      |
| Sucker Punch      | Yes   | Yes    | Yes        | No         | No      | No      |
| Active Job Async  | Yes   | Yes    | Yes        | No         | No      | No      |
| Active Job Inline | No    | Yes    | N/A        | N/A        | N/A     | N/A     |
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Sneakers and RabbitMQ are a perfect fit for my specific application: we leverage
the highly available queues and versatile AMQP semantics for several use-cases,
involving services written in other languages than Ruby. For example, our
RabbitMQ exposes a MQTT frontend that collects metrics from our IoT devices, and
makes it possible to implement several decoupled data processing pipelines,
something cumbersome to implement with Rails-specific queuing mechanisms.
Therefore, changing the queue backend just for this feature was not a desireable
option: I decided to implement the missing feature instead, and I will show you
how.&lt;/p&gt;

&lt;p&gt;Luckily, there exists a well designed RabbitMQ plugin that does exactly what I
needed, so I just had to write the adapter logic for it. The plugin is called
&lt;a href="https://github.com/rabbitmq/rabbitmq-delayed-message-exchange"&gt;&lt;code&gt;rabbitmq_delayed_message_exchange&lt;/code&gt;&lt;/a&gt;,
and can be easily added to an existing RabbitMQ installation by downloading
the binary build, putting it into the plugins directory, and enabling it.&lt;/p&gt;

&lt;p&gt;The plugin is well-documented and fairly straightforward to use, for those
familiar with RabbitMQ and AMQP. In order to schedule delayed messages, one
just has to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Declare an exchange with type &lt;code&gt;x-delayed-message&lt;/code&gt;, and an extra
 &lt;code&gt;x-delayed-type&lt;/code&gt; header to indicate the desired routing semantic to follow
 after the delay elapses (like &amp;ldquo;direct&amp;rdquo;, or &amp;ldquo;topic&amp;rdquo;, etc.).&lt;/li&gt;
&lt;li&gt;Publish messages on that exchange, providing an &lt;code&gt;x-delay&lt;/code&gt; header indicating
 the desired delay in milliseconds.&lt;/li&gt;
&lt;li&gt;Queues bound to the exchange will then receive the message after the given
 delay elapses, and from this point on everything works according to the
 standard AMQP protocol.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What was missing was only the integration between this plugin and our
Rails + ActiveJob + Sneakers setup. Essentially, I needed to publish jobs that
specify a &lt;code&gt;wait: &amp;lt;seconds&amp;gt;&lt;/code&gt; option using a custom publisher that uses a
&lt;code&gt;x-delayed-message&lt;/code&gt; exchange and sets the &lt;code&gt;x-delay&lt;/code&gt; header, while leaving jobs
that do not specify a delay to the standard Sneakers publisher.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code I ended up writing, using &lt;code&gt;ActiveSupport::Concern&lt;/code&gt; to make it
easily pluggable to an existing &lt;code&gt;ActiveJob&lt;/code&gt; class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;table class="rouge-table"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="rouge-gutter gl"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
&lt;/pre&gt;&lt;/td&gt;&lt;td class="rouge-code"&gt;&lt;pre&gt;&lt;span class="nb"&gt;require&lt;/span&gt; &lt;span class="s1"&gt;'sneakers'&lt;/span&gt;

&lt;span class="k"&gt;module&lt;/span&gt; &lt;span class="nn"&gt;PerformDelayed&lt;/span&gt;
  &lt;span class="kp"&gt;extend&lt;/span&gt; &lt;span class="no"&gt;ActiveSupport&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="no"&gt;Concern&lt;/span&gt;

  &lt;span class="n"&gt;class_methods&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;perform_later&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="c1"&gt;# If a job does not specify `:wait`, just process it normally&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;super&lt;/span&gt; &lt;span class="k"&gt;unless&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;is_a?&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;:wait&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

      &lt;span class="c1"&gt;# Otherwise, publish it on the delayed message exchange setting the&lt;/span&gt;
      &lt;span class="c1"&gt;# `x-delay` header&lt;/span&gt;
      &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;without&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:wait&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="n"&gt;delayed_publisher&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;publish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="no"&gt;ActiveSupport&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="no"&gt;JSON&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;serialize&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="ss"&gt;headers: &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s1"&gt;'x-delay'&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;:wait&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nf"&gt;to_i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="ss"&gt;routing_key: &lt;/span&gt;&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;queue_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

      &lt;span class="c1"&gt;# Log in the usual ActiveJob format, to make debugging easier&lt;/span&gt;
      &lt;span class="no"&gt;Rails&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"[ActiveJob] &lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="nb"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; (Job ID: &lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;job_id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;) to "&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"PerformDelayed(&lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;queue_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;) with arguments: "&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="ss"&gt;:inspect&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nf"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;', '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, &lt;/span&gt;&lt;span class="si"&gt;#{&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;inspect&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;delayed_publisher&lt;/span&gt;
      &lt;span class="c1"&gt;# Cache the publisher at the class level, so that all job instances of&lt;/span&gt;
      &lt;span class="c1"&gt;# the including class will reuse the same&lt;/span&gt;
      &lt;span class="vi"&gt;@delayed_publisher&lt;/span&gt; &lt;span class="o"&gt;||=&lt;/span&gt; &lt;span class="n"&gt;create_delayed_publisher!&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;

    &lt;span class="kp"&gt;private&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_delayed_publisher!&lt;/span&gt;
      &lt;span class="c1"&gt;# Idempotently create the delayed message exchange and the queue, then&lt;/span&gt;
      &lt;span class="c1"&gt;# create a publisher for them&lt;/span&gt;
      &lt;span class="n"&gt;opts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="ss"&gt;exchange: &lt;/span&gt;&lt;span class="s1"&gt;'delayed.exchange'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="ss"&gt;exchange_options: &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="ss"&gt;type: &lt;/span&gt;&lt;span class="s1"&gt;'x-delayed-message'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="ss"&gt;arguments: &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s1"&gt;'x-delayed-type'&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;'direct'&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
          &lt;span class="ss"&gt;durable: &lt;/span&gt;&lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="ss"&gt;auto_delete: &lt;/span&gt;&lt;span class="kp"&gt;false&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="n"&gt;delayed_publisher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="no"&gt;Sneakers&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="no"&gt;Publisher&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;delayed_publisher&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;ensure_connection!&lt;/span&gt;
      &lt;span class="n"&gt;queue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delayed_publisher&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;queue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;queue_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="no"&gt;Sneakers&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="no"&gt;CONFIG&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="ss"&gt;:queue_options&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
      &lt;span class="n"&gt;queue&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delayed_publisher&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;exchange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;routing_key: &lt;/span&gt;&lt;span class="n"&gt;queue_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;delayed_publisher&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In order to schedule a delayed job, I then just have to include this concern in
the job class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;table class="rouge-table"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="rouge-gutter gl"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class="rouge-code"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SomeJob&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="no"&gt;ApplicationJob&lt;/span&gt;
  &lt;span class="kp"&gt;include&lt;/span&gt; &lt;span class="no"&gt;PerformDelayed&lt;/span&gt;
  &lt;span class="n"&gt;queue_as&lt;/span&gt; &lt;span class="ss"&gt;:default&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;perform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# actually perform work&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I can now schedule jobs with a given delay using the standard &lt;code&gt;wait: &amp;lt;seconds&amp;gt;&lt;/code&gt;
option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;table class="rouge-table"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="rouge-gutter gl"&gt;&lt;pre class="lineno"&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class="rouge-code"&gt;&lt;pre&gt;&lt;span class="no"&gt;SomeJob&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;perform_later&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;some_argument&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;wait: &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;minutes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;RabbitMQ is an excellent messaging queue system (although merits and demerits,
when speaking about technologies, are
&lt;a href="/blog/2017/11/13/on-software-engineering-and-trade-offs.html"&gt;always contextual&lt;/a&gt;,
so be skeptical of anyone saying &amp;ldquo;if you don&amp;rsquo;t use X, you&amp;rsquo;re doing it wrong&amp;rdquo;).
Sneakers offers a nice adapter to use RabbitMQ as a backend for &lt;code&gt;ActiveJob&lt;/code&gt; in
Ruby on Rails, but unfortunately it does not implement delayed jobs out of the
box.&lt;/p&gt;

&lt;p&gt;Luckily, this feature is easy to implement, as shown in this post. With a little
more effort, one can also implement support for the &lt;code&gt;wait_until: &amp;lt;point in
time&amp;gt;&lt;/code&gt; option, which is left as an excercise to the reader :)&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>On Engineering Software and Trade-Offs</title>
    <link rel="alternate" href="http://blog.url.com/blog/2017/11/13/on-software-engineering-and-trade-offs.html"/>
    <id>http://blog.url.com/blog/2017/11/13/on-software-engineering-and-trade-offs.html</id>
    <published>2017-11-13T14:36:00+01:00</published>
    <updated>2018-08-01T12:47:43+02:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;As software developers, we tend to be quite opinionated about our tools and
techniques. Our own specific education and experiences shape our preferences,
and we often identify with several schools of thought. We might be, for example,
advocates of functional programming, practitioners of test driven development,
or microservices enthusiasts. On top of this, we all have our preferences when
it comes to technology: our favorite programming languages, databases,
infrastructure… These preferences shape our identity as software developers,
and exert a profound influence on the way we think: we view problems through
those lenses, and determine the way we envision solutions.&lt;/p&gt;

&lt;p&gt;On one hand, this is often a good thing: these disciplines provide us with a
useful model of the programming reality and with ways to navigate it. On the
other, strong beliefs might cause us to end up stuck defending our own stance,
unable to see the merits of a different view point. It is not unusual to witness
developers belonging to different schools of thought vehemently argue over some
technical decision, each strenuously defending their own view of the
(programming) World.&lt;/p&gt;

&lt;p&gt;While there is nothing wrong in recognizing the merits of a technique and in
adopting it, we should never forget one fundamental point: &lt;em&gt;pretty much every
technical discipline or tool embodies a trade-off. Techniques and technologies
are solutions to specific problems, and their merits or flaws are never
absolute, but always bound to the context.&lt;/em&gt; Being aware of where these
trade-offs lie is necessary not only to operate the right choices of tech, but
also to maintain an open and flexible mind, capable of changing approach when
necessary. Moreover, knowing where costs and benefits lie, provide us with the
opportunity to innovate in ways that shift the trade-offs in a better direction.&lt;/p&gt;

&lt;h2&gt;Static vs. Dynamic Typing&lt;/h2&gt;

&lt;p&gt;One prominent example of something that many software developers hold strong
opinions about, is statically versus dynamically typed languages. Advocates of
static typing on one hand often maintain that strong typing is absolutely
necessary for any serious project, while &amp;ndash; on the other hand &amp;ndash; users of
dynamically typed languages regard static typing as a tedious and mostly
unnecessary ceremony.&lt;/p&gt;

&lt;p&gt;Despite these strong beliefs, evidence shows that both approaches can be
extremely successful, ruling out a single objective winner of the diatribe. If
dynamic typing cannot scale, how can we explain the existence of numerous large
projects written in JavaScript, Python, Ruby, etc.? Let alone the immense
success of C as a system programming language, which, while definitely not
dynamically typed, can hardly be considered a strong type system. Equally, if it
was true that static typing only hinders productivity and expressiveness, how
could we account for the vast number of widely adopted strongly type languages,
and their evident success on the field? The situation is not different if we
turn our attention to demerits: no matter how opinionated we might be on one
side or the other, we can at least agree that terrible code can be written in
any practical language, no matter the type system.&lt;/p&gt;

&lt;p&gt;Therefore, instead of adopting an absolute view point, let&amp;rsquo;s try to focus
on the trade-offs of these two approaches. One way to look at it, is that static
typing reduces the number of runtime errors, at the cost of requiring more
effort to express solutions in code. Seen through these lenses, we can start
appreciating how contextual the specific merits of both disciplines are. The
cost of runtime errors is indeed vastly different for each specific application.&lt;/p&gt;

&lt;p&gt;In a web application for example, &amp;ldquo;runtime&amp;rdquo; often means the development machine
on which code is written, or the automated test environment. In a non-critical
application adopting continuous delivery, even when the occasional bug slips to
production, it can easily be reverted or patched with a new deployment. In these
situations, favoring a language that makes it convenient to write automated
tests, and shortens the test-code-deploy cycle, might often be the right choice.
As a counter example, native mobile applications and embedded software follow
release cycles that make it costly to deploy a fix to all users, if a runtime
bug is discovered. In this cases, a strong type system can help catching defects
and inconsistencies before it&amp;rsquo;s too late, and is worth some more effort to get
our software to compile.&lt;/p&gt;

&lt;p&gt;One objection to this line of reasoning could be that we should strive to
minimize defects, no matter if they are more or less costly. That is of course
true, but the point is that this minimization is subject to a cost structure,
and the optimal solution depends on those costs. If that wasn&amp;rsquo;t the case, we
would witness a world of absolutely bug-free software. The reality is quite
different, and our job as engineers comes necessarily with a fair share of risk
management considerations.&lt;/p&gt;

&lt;p&gt;Of course there are other trade-offs at play, such as the extent to which IDEs
can help us, versus the redundancy of the hints we have to give to compilers for
them to help us catching inconsistencies. Again, the point is that being aware
of them makes us better equipped to make informed decisions.&lt;/p&gt;

&lt;p&gt;There are numerous examples of innovations on both sides that all rely on an
awareness of these trade-offs and a conscious effort to improve on them. Type
inference is an effort to reduce the additional effort of writing statically
typed code, and got to the point that some static languages, like
&lt;a href="https://crystal-lang.org/"&gt;Crystal&lt;/a&gt;, hardly require any type annotation at all.
On the other hand, there are dynamic languages that add some static analysis
capabilities to their toolbox, as seen in
&lt;a href="http://erlang.org/doc/man/dialyzer.html"&gt;Dialyzer&lt;/a&gt; for Erlang and Elixir, or
&lt;a href="https://flow.org/"&gt;Flow&lt;/a&gt; for JavaScript, making the static vs. dynamic typing
distinction more like a gradient of possibilities than a binary choice.&lt;/p&gt;

&lt;h2&gt;Microservices vs. Monolith&lt;/h2&gt;

&lt;p&gt;Another example of a polarizing diatribe is microservices versus monolithic
architectures. The term &amp;ldquo;monolith&amp;rdquo; is already subtly conveying an association
with something old and clumsy, almost prehistoric, testifying how heated the
debate is. Equally, the Internet is bubbling with examples of microservice
architectures gone awry. But once again, let&amp;rsquo;s try to steer the conversation
away from fruitless animosity by focusing on trade-offs of the respective
solutions.&lt;/p&gt;

&lt;p&gt;Microservices divide an application in separate and independent artifacts
communicating with each other passing messages through standardized interfaces,
usually (but not exclusively) implemented as HTTP APIs. As such, microservice
architectures make it easier to evolve and improve at the level of the single
service: as long as the interface stays the same, a service can be completely
rewritten without the other services even noticing. On the other hand,
microservices make the boundaries between services way more rigid: changing
those boundaries requires careful coordination between different services, and
often between different teams.&lt;/p&gt;

&lt;p&gt;Depending on the life cycle of a project, and on how stable the functional
boundaries are expected to be, this trade-off can change dramatically, making
one approach or the other preferable or problematic. This dynamic is not
exclusive of our industry: in management studies, this trade-off is known as
modularity vs. integrality, and there are plenty of case studies showing how
each approach has proved beneficial or detrimental to companies in different
contexts.&lt;/p&gt;

&lt;p&gt;Once again, being aware of these costs and benefits, makes it possible to
consider the context in which we operate our decisions. Moreover, we often have
the possibility to shift the balance or hedge the risks, for example by adopting
a modular architecture even within a single service, or splitting our services
only where boundaries are well known and stable.&lt;/p&gt;

&lt;h2&gt;Distributed Systems vs. Centralized Systems&lt;/h2&gt;

&lt;p&gt;One final example is distributed systems versus centralized ones. There has been
a lot of innovation in the field of distributed systems in recent years, and we
have witnessed a proliferation of new decentralized solutions dealing with
storage, configuration management, messaging queues, and more. Proponents of
this approach outline the benefits of distribution when it comes to scaling and
resilience, but the wave of innovation brought also a certain disdain for
centralized solutions.&lt;/p&gt;

&lt;p&gt;Distributed systems are a welcome addition to our choice of technologies, but
once again, their advantages do not come without a cost. A distributed system
reduces the chances of a failure of the whole system, at the cost of having to
run and coordinate several nodes, therefore incurring in some overhead, and
increasing the chances that any single node will fail, requiring intervention.
This might seem like a reasonable trade-off to accept, but there are products or
teams where an occasional short downtime is more acceptable than a sustained
higher effort on operations.&lt;/p&gt;

&lt;p&gt;A centralized system is harder to scale, but on the other hand it is not
susceptible to network partitions, hence it is not subject to the &lt;a href="https://en.wikipedia.org/wiki/CAP_theorem"&gt;CAP
theorem&lt;/a&gt; and can be at the same time
available and consistent. Therefore, depending on the scale and requirements of
a particular project, different approaches are preferable.&lt;/p&gt;

&lt;p&gt;By keeping this trade-off in mind, and by knowing the specific context and
requirements of a project, we are able to make informed decisions, and to
consciously mitigate the risks that come with our technology of choice.
Furthermore, centralized systems can be backed-up and replicated, and
distributed systems can alleviate the operational burden by automating some
operations, and by adopting an application design that takes into consideration
from the beginning the limits of the system, when it comes to consistency or
availability.&lt;/p&gt;

&lt;h2&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In conclusion, the software development universe is full of polarizing
dichotomies, and engineers often have strong opinions: functional programming
versus object orientation, client-side versus server-side rendering, performance
versus maintainability, and so on. Experience should teach us that in each of
those dichotomies lies a trade-off, as well as different boundaries of
applicability. Reflecting on costs and benefits helps us keeping a flexible mind
and recognizing opportunities to adopt different strategies. Reminding ourselves
that every solution is contextual and never absolute is an exercise of
intellectual honesty, if we strive to be well-rounded engineers. Finally,
focusing on trade-offs and context, rather than on position and beliefs, makes
technical discussions a lot more enjoyable and less prone to end with an
impasse.&lt;/p&gt;
</content>
  </entry>
</feed>
